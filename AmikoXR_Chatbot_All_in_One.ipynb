{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ba5258",
   "metadata": {},
   "source": [
    "# To comply with cinfidentiality obligations, the code has been simplified to demonstrate fundamental concepts of an AI agent chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4457552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: LangChain in c:\\users\\kasra\\anaconda3\\lib\\site-packages (0.2.15)\n",
      "Collecting LangChain\n",
      "  Obtaining dependency information for LangChain from https://files.pythonhosted.org/packages/0d/29/635343c0d155997569b544d26da5a2a9ebade2423baffc9cd6066b01a386/langchain-0.2.16-py3-none-any.whl.metadata\n",
      "  Downloading langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: langchain_openai in c:\\users\\kasra\\anaconda3\\lib\\site-packages (0.1.23)\n",
      "Requirement already satisfied: OpenAI in c:\\users\\kasra\\anaconda3\\lib\\site-packages (1.43.0)\n",
      "Collecting OpenAI\n",
      "  Obtaining dependency information for OpenAI from https://files.pythonhosted.org/packages/99/ae/e8fb328fc0fc20ae935950b1f7160de8e2631a5997c2398c9b8a8cc502f8/openai-1.44.0-py3-none-any.whl.metadata\n",
      "  Downloading openai-1.44.0-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: Datasets in c:\\users\\kasra\\anaconda3\\lib\\site-packages (2.21.0)\n",
      "Requirement already satisfied: qdrant-client in c:\\users\\kasra\\anaconda3\\lib\\site-packages (1.11.1)\n",
      "Requirement already satisfied: Tiktoken in c:\\users\\kasra\\anaconda3\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from LangChain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from LangChain) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from LangChain) (3.10.3)\n",
      "Collecting langchain-core<0.3.0,>=0.2.38 (from LangChain)\n",
      "  Obtaining dependency information for langchain-core<0.3.0,>=0.2.38 from https://files.pythonhosted.org/packages/1c/e4/501fbe904530dad6ed80f03b188d7602081560dd5cc0bcf0b3c51778c314/langchain_core-0.2.38-py3-none-any.whl.metadata\n",
      "  Downloading langchain_core-0.2.38-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from LangChain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from LangChain) (0.1.100)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from LangChain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from LangChain) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from LangChain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from LangChain) (8.5.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from OpenAI) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from OpenAI) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from OpenAI) (0.25.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from OpenAI) (0.5.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from OpenAI) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from OpenAI) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from OpenAI) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from Datasets) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from Datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from Datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from Datasets) (1.5.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from Datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from Datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.6.1,>=2023.1.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from Datasets) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from Datasets) (0.24.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from Datasets) (24.1)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from qdrant-client) (1.65.5)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from qdrant-client) (1.65.5)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from qdrant-client) (2.10.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from qdrant-client) (1.26.16)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from Tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->LangChain) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->LangChain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->LangChain) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->LangChain) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->LangChain) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->LangChain) (1.8.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->OpenAI) (3.4)\n",
      "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (5.27.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (68.2.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->OpenAI) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->OpenAI) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->OpenAI) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->OpenAI) (4.1.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.38->LangChain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->LangChain) (3.10.7)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client) (305.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->LangChain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->LangChain) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from requests<3,>=2->LangChain) (2.0.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->LangChain) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from tqdm>4->OpenAI) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from pandas->Datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from pandas->Datasets) (2023.4)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from h2<5,>=3->httpx<1,>=0.23.0->OpenAI) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from h2<5,>=3->httpx<1,>=0.23.0->OpenAI) (4.0.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->LangChain) (2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->Datasets) (1.16.0)\n",
      "Downloading langchain-0.2.16-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.2/1.0 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.6/1.0 MB 5.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.7/1.0 MB 4.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 0.7/1.0 MB 226.6 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 0.7/1.0 MB 226.6 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 0.7/1.0 MB 225.2 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 0.7/1.0 MB 225.2 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 0.7/1.0 MB 226.1 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 0.7/1.0 MB 224.9 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 0.8/1.0 MB 230.0 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 0.9/1.0 MB 264.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.0/1.0 MB 295.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 295.0 kB/s eta 0:00:00\n",
      "Downloading openai-1.44.0-py3-none-any.whl (367 kB)\n",
      "   ---------------------------------------- 0.0/367.8 kB ? eta -:--:--\n",
      "   ------------------------------- -------- 286.7/367.8 kB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 367.8/367.8 kB 4.6 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.2.38-py3-none-any.whl (396 kB)\n",
      "   ---------------------------------------- 0.0/396.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 396.4/396.4 kB 12.1 MB/s eta 0:00:00\n",
      "Installing collected packages: OpenAI, langchain-core, LangChain\n",
      "  Attempting uninstall: OpenAI\n",
      "    Found existing installation: openai 1.43.0\n",
      "    Uninstalling openai-1.43.0:\n",
      "      Successfully uninstalled openai-1.43.0\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.2.35\n",
      "    Uninstalling langchain-core-0.2.35:\n",
      "      Successfully uninstalled langchain-core-0.2.35\n",
      "  Attempting uninstall: LangChain\n",
      "    Found existing installation: langchain 0.2.15\n",
      "    Uninstalling langchain-0.2.15:\n",
      "      Successfully uninstalled langchain-0.2.15\n",
      "Successfully installed LangChain-0.2.16 OpenAI-1.44.0 langchain-core-0.2.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pymemgpt 0.3.22 requires protobuf==3.20.0, but you have protobuf 5.27.3 which is incompatible.\n",
      "pymemgpt 0.3.22 requires tiktoken<0.6.0,>=0.5.1, but you have tiktoken 0.7.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environ({'ALLUSERSPROFILE': 'C:\\\\ProgramData', 'APPDATA': 'C:\\\\Users\\\\Kasra\\\\AppData\\\\Roaming', 'COMMONPROGRAMFILES': 'C:\\\\Program Files\\\\Common Files', 'COMMONPROGRAMFILES(X86)': 'C:\\\\Program Files (x86)\\\\Common Files', 'COMMONPROGRAMW6432': 'C:\\\\Program Files\\\\Common Files', 'COMPUTERNAME': 'DESKTOP-KVL4TLC', 'COMSPEC': 'C:\\\\WINDOWS\\\\system32\\\\cmd.exe', 'DRIVERDATA': 'C:\\\\Windows\\\\System32\\\\Drivers\\\\DriverData', 'EFC_7216': '1', 'HOMEDRIVE': 'C:', 'HOMEPATH': '\\\\Users\\\\Kasra', 'LOCALAPPDATA': 'C:\\\\Users\\\\Kasra\\\\AppData\\\\Local', 'LOGONSERVER': '\\\\\\\\DESKTOP-KVL4TLC', 'NUMBER_OF_PROCESSORS': '4', 'ONEDRIVE': 'C:\\\\Users\\\\Kasra\\\\OneDrive - stevens.edu', 'ONEDRIVECOMMERCIAL': 'C:\\\\Users\\\\Kasra\\\\OneDrive - stevens.edu', 'ONEDRIVECONSUMER': 'C:\\\\Users\\\\Kasra\\\\OneDrive', 'ONLINESERVICES': 'Online Services', 'OS': 'Windows_NT', 'PATH': 'C:\\\\Users\\\\Kasra\\\\anaconda3;C:\\\\Users\\\\Kasra\\\\anaconda3\\\\Library\\\\mingw-w64\\\\bin;C:\\\\Users\\\\Kasra\\\\anaconda3\\\\Library\\\\usr\\\\bin;C:\\\\Users\\\\Kasra\\\\anaconda3\\\\Library\\\\bin;C:\\\\Users\\\\Kasra\\\\anaconda3\\\\Scripts;C:\\\\Program Files (x86)\\\\Windows Resource Kits\\\\Tools\\\\;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\WINDOWS\\\\System32\\\\OpenSSH\\\\;C:\\\\Program Files\\\\Microsoft SQL Server\\\\Client SDK\\\\ODBC\\\\170\\\\Tools\\\\Binn\\\\;C:\\\\Program Files (x86)\\\\Microsoft SQL Server\\\\150\\\\Tools\\\\Binn\\\\;C:\\\\Program Files\\\\Microsoft SQL Server\\\\150\\\\Tools\\\\Binn\\\\;C:\\\\Program Files\\\\Microsoft SQL Server\\\\150\\\\DTS\\\\Binn\\\\;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\Program Files\\\\HP\\\\HP One Agent;C:\\\\Program Files (x86)\\\\Microsoft SQL Server\\\\150\\\\DTS\\\\Binn\\\\;C:\\\\Users\\\\Kasra\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Program Files\\\\Azure Data Studio\\\\bin;C:\\\\Program Files (x86)\\\\MongoDB Atlas CLI\\\\;', 'PATHEXT': '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC', 'PLATFORMCODE': 'KV', 'PROCESSOR_ARCHITECTURE': 'AMD64', 'PROCESSOR_IDENTIFIER': 'Intel64 Family 6 Model 142 Stepping 12, GenuineIntel', 'PROCESSOR_LEVEL': '6', 'PROCESSOR_REVISION': '8e0c', 'PROGRAMDATA': 'C:\\\\ProgramData', 'PROGRAMFILES': 'C:\\\\Program Files', 'PROGRAMFILES(X86)': 'C:\\\\Program Files (x86)', 'PROGRAMW6432': 'C:\\\\Program Files', 'PSMODULEPATH': 'C:\\\\Program Files\\\\WindowsPowerShell\\\\Modules;C:\\\\WINDOWS\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\Modules;C:\\\\Program Files (x86)\\\\Microsoft SQL Server\\\\150\\\\Tools\\\\PowerShell\\\\Modules\\\\', 'PUBLIC': 'C:\\\\Users\\\\Public', 'REGIONCODE': 'EMEA', 'SESSIONNAME': 'Console', 'SYSTEMDRIVE': 'C:', 'SYSTEMROOT': 'C:\\\\WINDOWS', 'TEMP': 'C:\\\\Users\\\\Kasra\\\\AppData\\\\Local\\\\Temp', 'TMP': 'C:\\\\Users\\\\Kasra\\\\AppData\\\\Local\\\\Temp', 'USERDOMAIN': 'DESKTOP-KVL4TLC', 'USERDOMAIN_ROAMINGPROFILE': 'DESKTOP-KVL4TLC', 'USERNAME': 'Kasra', 'USERPROFILE': 'C:\\\\Users\\\\Kasra', 'WINDIR': 'C:\\\\WINDOWS', 'ZES_ENABLE_SYSMAN': '1', '__PSLOCKDOWNPOLICY': '0', 'CONDA_PREFIX': 'C:\\\\Users\\\\Kasra\\\\anaconda3', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'JPY_INTERRUPT_EVENT': '4996', 'IPY_INTERRUPT_EVENT': '4996', 'JPY_PARENT_PID': '4856', 'TERM': 'xterm-color', 'CLICOLOR': '1', 'FORCE_COLOR': '1', 'CLICOLOR_FORCE': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://matplotlib_inline.backend_inline'})\n",
      "Machine learning is a subset of artificial intelligence that focuses on developing algorithms and models that enable computers to learn from data and make predictions or decisions without being explicitly programmed. This is achieved through the use of statistical techniques and algorithms that allow the computer to identify patterns and relationships in the data and use this information to make predictions or decisions.\n",
      "\n",
      "There are different types of machine learning algorithms, including supervised learning, unsupervised learning, and reinforcement learning. In supervised learning, the algorithm is trained on a labeled dataset, where each data point is associated with a specific label or outcome. The algorithm learns to make predictions by identifying patterns in the input data and the corresponding labels.\n",
      "\n",
      "Unsupervised learning, on the other hand, involves training the algorithm on an unlabeled dataset, where the goal is to identify patterns or relationships in the data without any predefined labels. Clustering and dimensionality reduction are common techniques used in unsupervised learning.\n",
      "\n",
      "Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with an environment and receiving feedback in the form of rewards or penalties. The agent learns to maximize its cumulative reward over time by exploring different actions and learning from the outcomes.\n",
      "\n",
      "Machine learning is used in a wide range of applications, including image and speech recognition, natural language processing, recommendation systems, and autonomous vehicles, among others. It has the potential to revolutionize industries and improve efficiency and decision-making processes across various domains.\n",
      "The main difference between supervised and unsupervised learning lies in the presence of labeled data. In supervised learning, the algorithm is trained on a labeled dataset, where each data point is associated with a specific label or outcome that the algorithm needs to predict. The goal of supervised learning is to learn a mapping from input data to output labels based on the training examples provided.\n",
      "\n",
      "On the other hand, unsupervised learning involves training the algorithm on an unlabeled dataset, where there are no predefined labels or outcomes. The goal of unsupervised learning is to identify patterns or relationships in the data without the need for labeled examples. Unsupervised learning algorithms aim to discover hidden structures or clusters in the data, such as grouping similar data points together or reducing the dimensionality of the data.\n",
      "\n",
      "In supervised learning, the algorithm learns to make predictions by minimizing the error between its predictions and the actual labels in the training data. The algorithm is provided with feedback on its predictions during training, allowing it to adjust its parameters and improve its accuracy over time.\n",
      "\n",
      "In unsupervised learning, the algorithm does not have access to labeled examples and must rely on the inherent structure of the data to learn meaningful patterns. Unsupervised learning algorithms often use clustering or dimensionality reduction techniques to uncover hidden patterns in the data.\n",
      "\n",
      "In summary, supervised learning requires labeled data for training and involves predicting output labels based on input features, while unsupervised learning does not require labeled data and focuses on discovering patterns or relationships in the input data without predefined outcomes.\n",
      "Mistral 7B is a high-performance microprocessor designed by SiPearl, a European semiconductor company specializing in developing processors for high-performance computing and artificial intelligence applications. The Mistral 7B is based on the RISC-V instruction set architecture, which is an open standard instruction set architecture that is designed to be simple, efficient, and customizable.\n",
      "\n",
      "The Mistral 7B processor is designed for use in exascale supercomputers and other high-performance computing systems that require high computational power and energy efficiency. It features a multi-core design with advanced features such as out-of-order execution, advanced branch prediction, and support for advanced vector instructions.\n",
      "\n",
      "One of the key benefits of the Mistral 7B processor is its scalability and performance efficiency, which makes it well-suited for a wide range of applications in the fields of scientific computing, artificial intelligence, and data analytics. The RISC-V architecture allows for customization and optimization of the processor for specific workloads, making it a versatile and flexible option for high-performance computing systems.\n",
      "\n",
      "Overall, the Mistral 7B processor stands out for its high performance, energy efficiency, and scalability, making it a promising option for next-generation supercomputing and AI applications.\n",
      "I'm sorry, but I don't have specific information about Mistral 7B or LangChain in the provided contexts. If you have any other questions or need assistance with a different topic, feel free to ask!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qdrant URL: https://b5512ba6-9796-4afa-974d-62bda96948c3.europe-west3-0.gcp.cloud.qdrant.io/\n",
      "API Key: b2aZn45Vr3_Olldq5xk7iAPmaoRzvifPb0DHayKBGZTKIoWlQS2k9Q\n",
      "Connecting to Qdrant at: https://b5512ba6-9796-4afa-974d-62bda96948c3.europe-west3-0.gcp.cloud.qdrant.io/\n",
      "Using API Key: b2aZn45Vr3_Olldq5xk7iAPmaoRzvifPb0DHayKBGZTKIoWlQS2k9Q\n",
      "Using the contexts below, answer the query:\n",
      "    \n",
      "    Contexts:\n",
      "    Mistral 7B\n",
      "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\n",
      "Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\n",
      "Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\n",
      "Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\n",
      "William El Sayed\n",
      "Abstract\n",
      "We introduce Mistral 7B, a 7–billion-parameter language model engineered for\n",
      "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
      "model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
      "model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
      "leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
      "window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
      "reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
      "Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\n",
      "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
      "Code: https://github.com/mistralai/mistral-src\n",
      "Mistral 7B\n",
      "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\n",
      "Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\n",
      "Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\n",
      "Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\n",
      "William El Sayed\n",
      "Abstract\n",
      "We introduce Mistral 7B, a 7–billion-parameter language model engineered for\n",
      "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
      "model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
      "model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
      "leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
      "window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
      "reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
      "Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\n",
      "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
      "Code: https://github.com/mistralai/mistral-src\n",
      "GQA significantly accelerates the inference speed, and also reduces the memory requirement during\n",
      "decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\n",
      "applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\n",
      "computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\n",
      "collectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023\n",
      "Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\n",
      "implementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\n",
      "or Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\n",
      "also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\n",
      "a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\n",
      "model fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\n",
      "Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping\n",
      "large language models efficient. Through our work, our aim is to help the community create more\n",
      "    \n",
      "    Query: What is so special about Mistral 7B?\n",
      "Mistral 7B is a 7-billion-parameter language model that has been engineered for superior performance and efficiency. It stands out due to several key features:\n",
      "\n",
      "1. **Outstanding Performance**: Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks and even surpasses the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. This demonstrates its exceptional capabilities in various tasks.\n",
      "\n",
      "2. **Efficiency**: The model leverages grouped-query attention (GQA) for faster inference and sliding window attention (SWA) to effectively handle sequences of arbitrary length with reduced inference cost. These attention mechanisms significantly enhance the efficiency of Mistral 7B.\n",
      "\n",
      "3. **Inference Speed and Memory Efficiency**: GQA accelerates the inference speed and reduces the memory requirement during decoding, allowing for higher batch sizes and throughput. This is critical for real-time applications where speed is essential.\n",
      "\n",
      "4. **Handling Longer Sequences**: SWA is designed to handle longer sequences more effectively at a reduced computational cost. This addresses a common limitation in large language models and contributes to Mistral 7B's enhanced performance.\n",
      "\n",
      "5. **Ease of Deployment and Fine-Tuning**: Mistral 7B is released under the Apache 2.0 license, accompanied by a reference implementation for easy deployment locally or on cloud platforms. It is crafted for ease of fine-tuning across various tasks, showcasing its adaptability and superior performance.\n",
      "\n",
      "6. **Balancing Performance and Efficiency**: Mistral 7B represents a significant step in balancing the goals of achieving high performance while keeping large language models efficient. This is important for ensuring that the model can deliver exceptional results without compromising on efficiency.\n",
      "\n",
      "Overall, Mistral 7B's combination of superior performance, efficiency, ease of deployment, and adaptability makes it a standout language model in the field.\n",
      "Requirement already satisfied: langchain-groq in c:\\users\\kasra\\anaconda3\\lib\\site-packages (0.1.9)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from langchain-groq) (0.10.0)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.26 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from langchain-groq) (0.2.38)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (0.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (0.1.100)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (24.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (8.5.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.26->langchain-groq) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (3.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (2.32.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kasra\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (1.26.16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral 7B is a 7-billion parameter language model that is engineered for superior performance and efficiency. It outperforms the best open 13B model (Llama 2) across all evaluated benchmarks and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Mistral 7B uses grouped-query attention (GQA) for faster inference and sliding window attention (SWA) to handle sequences of arbitrary length with reduced inference cost. It also provides a model fine-tuned to follow instructions, Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model on both human and automated benchmarks. Mistral 7B is released under the Apache 2.0 license and is easy to deploy and integrate with cloud platforms and Hugging Face. It is also designed for ease of fine-tuning across a wide range of tasks. Overall, Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping large language models efficient.\n"
     ]
    }
   ],
   "source": [
    "# AMikoXR Chatbot using LangChain, Open AI, RAG, Qdrant, Groq  \n",
    "\n",
    "### Install Libraries\n",
    "\n",
    "!pip install -U \\\n",
    "LangChain \\\n",
    "langchain_openai \\\n",
    "OpenAI \\\n",
    "Datasets \\\n",
    "qdrant-client \\\n",
    "Tiktoken\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv('./.env')\n",
    "\n",
    "print(os.environ)\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model='gpt-3.5-turbo',\n",
    "    openai_api_key=''\n",
    ")\n",
    "\n",
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I would like to understand machine learnin.\")\n",
    "]\n",
    "\n",
    "res= chat.invoke(messages)\n",
    "res\n",
    "\n",
    "print(res.content)\n",
    "\n",
    "messages.append(res)\n",
    "\n",
    "prompt = HumanMessage(\n",
    "    content=\"Whats the difference between supervised and unsupervised\"\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "\n",
    "res= chat.invoke(messages)\n",
    "print(res.content)\n",
    "\n",
    "messages.append(res)\n",
    "\n",
    "prompt = HumanMessage(\n",
    "    content=\"What is so special about Mistral 7B?\"\n",
    ")\n",
    "messages.append(prompt)\n",
    "res = chat.invoke(messages)\n",
    "\n",
    "print(res.content)\n",
    "\n",
    "llmchain_information= [\n",
    "    \"LLMChain is an AI model that combines the capabilities of Large Language Models (LLMs) with blockchain technology. It's a framework that enables the creation of decentralized, transparent, and secure AI applications.\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(llmchain_information) \n",
    "\n",
    "query = \"Can you tell me about LLMChain in LangChain?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below to answer the question.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Question: {query}\"\"\"\n",
    "\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat.invoke(messages)\n",
    "\n",
    "print(res.content)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"infoslack/mistral-7b-arxiv-paper-chunked\", split=\"train\")\n",
    "\n",
    "dataset\n",
    "\n",
    "dataset[0]\n",
    "\n",
    "data = dataset.to_pandas()\n",
    "\n",
    "data.head()\n",
    "\n",
    "docs = data[['chunk','source']]\n",
    "docs.head()\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embed_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=\"\"\n",
    ")\n",
    "\n",
    "texts = [\n",
    "    'this is one chunk',\n",
    "    'this is the second chunk of text'\n",
    "]\n",
    "\n",
    "res = embed_model.embed_documents(texts)\n",
    "len(res), len(res[0])\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"infoslack/mistral-7b-arxiv-paper-chunked\", split=\"train\")\n",
    "\n",
    "dataset\n",
    "\n",
    "dataset[0]\n",
    "\n",
    "data = dataset.to_pandas()\n",
    "\n",
    "data\n",
    "\n",
    "docs = data[['chunk','source']]\n",
    "docs.head()\n",
    "\n",
    "#RAG\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(docs, page_content_column=\"chunk\")\n",
    "documents = loader.load()\n",
    "\n",
    "documents[0]\n",
    "\n",
    "documents[0].metadata\n",
    "\n",
    "# Set the environment variables\n",
    "os.environ['QDRANT_URL'] = ''\n",
    "os.environ['QDRANT_API_KEY'] = ''\n",
    "\n",
    "print(\"Qdrant URL:\", os.getenv('QDRANT_URL'))\n",
    "print(\"API Key:\", os.getenv('QDRANT_API_KEY'))\n",
    "\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Initialize Embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=\"\"\n",
    ")\n",
    "\n",
    "# Get URL and API Key\n",
    "url = os.getenv('QDRANT_URL')\n",
    "api_key = os.getenv('QDRANT_API_KEY')\n",
    "\n",
    "# Debugging output\n",
    "print(\"Connecting to Qdrant at:\", url)\n",
    "print(\"Using API Key:\", api_key)\n",
    "\n",
    "# Ensure URL is valid\n",
    "if not url:\n",
    "    print(\"Error: Qdrant URL is not set.\")\n",
    "    exit(1)\n",
    "\n",
    "# Attempt to create Qdrant instance\n",
    "try:\n",
    "    qdrant = Qdrant.from_documents(\n",
    "        documents=documents,  # Ensure 'documents' is defined elsewhere\n",
    "        embedding=embeddings,\n",
    "        url=url,\n",
    "        collection_name='chatbot',\n",
    "        api_key=api_key\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Failed to connect to Qdrant:\", str(e))\n",
    "\n",
    "query = \"What is so special about Mistral 7B?\"\n",
    "qdrant.similarity_search(query, k=3)\n",
    "\n",
    "def custom_prompt(query: str):\n",
    "    results = qdrant.similarity_search(query, k=3)\n",
    "    source_knowledge = '\\n'.join([x.page_content for x in results])\n",
    "    augment_prompt = f\"\"\"Using the contexts below, answer the query:\n",
    "    \n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "    \n",
    "    Query: {query}\"\"\"\n",
    "    return augment_prompt\n",
    "\n",
    "print(custom_prompt(query))\n",
    "\n",
    "prompt = HumanMessage(\n",
    "    content=custom_prompt(query)\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat.invoke(messages)\n",
    "\n",
    "print(res.content)\n",
    "\n",
    "#Groq\n",
    "!pip install langchain-groq\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "chat = ChatGroq(temperature=0, model_name='mixtral-8x7b-32768', groq_api_key='')\n",
    "\n",
    "prompt = HumanMessage(\n",
    "    content=custom_prompt(query)\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "res = chat.invoke(messages)\n",
    "print(res.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b55a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c15ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b0812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
